{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870725c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting: 1. Load and Prepare datasets\n",
      "Ending: 1. Load and Prepare datasets\n",
      "Starting: 2. Preprocessing: 7-mer embedding\n",
      "Ending: 2. Preprocessing: 7-mer embedding\n",
      "Starting: 3. Assign split bins\n",
      "ðŸ“Š Number of rows in each set:\n",
      "  - Train: 8820055 rows\n",
      "  - Val: 1105069 rows\n",
      "  - Test: 1101982 rows\n",
      "\n",
      "ðŸ“ˆ Label distribution (percentage of label 0 and 1) in each set:\n",
      "  - Test:\n",
      "      â€¢ Label 0: 94.08%\n",
      "      â€¢ Label 1: 5.92%\n",
      "  - Train:\n",
      "      â€¢ Label 0: 95.59%\n",
      "      â€¢ Label 1: 4.41%\n",
      "  - Val:\n",
      "      â€¢ Label 0: 95.90%\n",
      "      â€¢ Label 1: 4.10%\n",
      "Ending: 3. Assign split bins\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0. Imports & Setup\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "torch.manual_seed(RNG)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 1. Load and Preparing datasets\n",
    "# =========================\n",
    "print(\"Starting: 1. Load and Prepare datasets\")\n",
    "reads_df = pd.read_csv(\"Dataset/dataset.csv\")\n",
    "\n",
    "columns_to_drop = ['Unnamed: 0']\n",
    "reads_df = reads_df.drop(columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "# sorry cherron i cannot with the colnames\n",
    "reads_df = reads_df.rename(columns={\n",
    "    'ID': 'transcript_id',\n",
    "    'POS': 'transcript_position',\n",
    "    'SEQ': '7mer'\n",
    "})\n",
    "reads_df['n_reads'] = reads_df.groupby(['transcript_id', 'transcript_position']).transform('size')\n",
    "\n",
    "\n",
    "print(\"Ending: 1. Load and Prepare datasets\")\n",
    "# =========================\n",
    "# 2. Preprocessing: 7-mer embedding\n",
    "# =========================\n",
    "print(\"Starting: 2. Preprocessing: 7-mer embedding\")\n",
    "\n",
    "def encode_drach_compact(seq):\n",
    "    \"\"\"\n",
    "    Compact one-hot encoding of a 7-mer centered on a DRACH motif.\n",
    "    Positions:\n",
    "    - 0: full one-hot (A,C,G,T) â†’ 4 dims\n",
    "    - 1: D (A,G,T) â†’ 3 dims\n",
    "    - 2: R (A,G)   â†’ 2 dims\n",
    "    - 3: A (fixed) â†’ 0 dims\n",
    "    - 4: C (fixed) â†’ 0 dims\n",
    "    - 5: H (A,C,T) â†’ 3 dims\n",
    "    - 6: full one-hot (A,C,G,T) â†’ 4 dims\n",
    "    Total: 16-dimensional vector\n",
    "    \"\"\"\n",
    "    encoding = []\n",
    "\n",
    "    base = seq[0]\n",
    "    encoding.extend(one_hot_base(base, ['A', 'C', 'G', 'T']))\n",
    "\n",
    "    base = seq[1]\n",
    "    encoding.extend(one_hot_base(base, ['A', 'G', 'T']))  # D\n",
    "\n",
    "    base = seq[2]\n",
    "    encoding.extend(one_hot_base(base, ['A', 'G']))       # R\n",
    "\n",
    "    # skip position 3 (always A)\n",
    "    # skip position 4 (always C)\n",
    "\n",
    "    base = seq[5]\n",
    "    encoding.extend(one_hot_base(base, ['A', 'C', 'T']))  # H\n",
    "\n",
    "    base = seq[6]\n",
    "    encoding.extend(one_hot_base(base, ['A', 'C', 'G', 'T']))\n",
    "\n",
    "    return np.array(encoding, dtype=np.float32)\n",
    "\n",
    "def one_hot_base(base, allowed):\n",
    "    \"\"\"One-hot encode base using only allowed bases.\"\"\"\n",
    "    vec = [0] * len(allowed)\n",
    "    if base in allowed:\n",
    "        vec[allowed.index(base)] = 1\n",
    "    return vec\n",
    "\n",
    "reads_df['7mer_emb'] = reads_df['7mer'].apply(encode_drach_compact)\n",
    "print(\"Ending: 2. Preprocessing: 7-mer embedding\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Assign split bins\n",
    "# =========================\n",
    "print(\"Starting: 3. Assign split bins\")\n",
    "\n",
    "def assign_set_type_by_gene(reads_df, split_ratios={'Train': 0.8, 'Val': 0.1, 'Test': 0.1}, random_state=42):\n",
    "    \"\"\"\n",
    "    Assigns each row in reads_df a 'set_type' of Train, Val, or Test,\n",
    "    ensuring all rows with the same gene_id are in the same set,\n",
    "    and total number of rows (not just genes) in each set matches desired ratios.\n",
    "    Label distribution is approximately balanced using a greedy strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Get stats per gene\n",
    "    gene_stats = (\n",
    "        reads_df\n",
    "        .groupby('gene_id')['label']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .rename(columns={0: 'label_0', 1: 'label_1'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    gene_stats['total'] = gene_stats['label_0'] + gene_stats['label_1']\n",
    "\n",
    "    # Shuffle genes for randomness\n",
    "    gene_stats = gene_stats.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Overall label distribution and target row counts\n",
    "    total_rows = gene_stats['total'].sum()\n",
    "    total_label_1 = gene_stats['label_1'].sum()\n",
    "    overall_pos_rate = total_label_1 / total_rows\n",
    "\n",
    "    target_rows = {k: total_rows * split_ratios[k] for k in split_ratios}\n",
    "\n",
    "    # Step 3: Initialize bins\n",
    "    bins = {\n",
    "        'Train': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "        'Val': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "        'Test': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "    }\n",
    "\n",
    "    def pick_bin():\n",
    "        # Find the bin with the biggest gap between current and target row count\n",
    "        diffs = {k: target_rows[k] - bins[k]['total'] for k in bins}\n",
    "        # Choose the bin that needs rows the most\n",
    "        return max(diffs, key=diffs.get)\n",
    "\n",
    "    # Step 4: Assign genes to bins to match row targets and label balance\n",
    "    for _, row in gene_stats.iterrows():\n",
    "        chosen_bin = pick_bin()\n",
    "        bins[chosen_bin]['genes'].append(row['gene_id'])\n",
    "        bins[chosen_bin]['label_0'] += row['label_0']\n",
    "        bins[chosen_bin]['label_1'] += row['label_1']\n",
    "        bins[chosen_bin]['total'] += row['total']\n",
    "\n",
    "    # Step 5: Map gene_id â†’ set_type\n",
    "    gene_to_set = {}\n",
    "    for set_name, bin_data in bins.items():\n",
    "        for gene_id in bin_data['genes']:\n",
    "            gene_to_set[gene_id] = set_name\n",
    "\n",
    "    reads_df['set_type'] = reads_df['gene_id'].map(gene_to_set)\n",
    "\n",
    "    return reads_df\n",
    "\n",
    "\n",
    "reads_df = assign_set_type_by_gene(reads_df)\n",
    "\n",
    "set_counts = reads_df['set_type'].value_counts()\n",
    "print(\"ðŸ“Š Number of rows in each set:\")\n",
    "for set_name, count in set_counts.items():\n",
    "    print(f\"  - {set_name}: {count} rows\")\n",
    "\n",
    "# Print label distribution per set (normalized)\n",
    "label_distributions = reads_df.groupby('set_type')['label'].value_counts(normalize=True).unstack()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Label distribution (percentage of label 0 and 1) in each set:\")\n",
    "for set_name in label_distributions.index:\n",
    "    label_0_pct = label_distributions.loc[set_name].get(0, 0) * 100\n",
    "    label_1_pct = label_distributions.loc[set_name].get(1, 0) * 100\n",
    "    print(f\"  - {set_name}:\")\n",
    "    print(f\"      â€¢ Label 0: {label_0_pct:.2f}%\")\n",
    "    print(f\"      â€¢ Label 1: {label_1_pct:.2f}%\")\n",
    "\n",
    "print(\"Ending: 3. Assign split bins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01da9f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Data Types:\n",
      "transcript_id           object\n",
      "transcript_position      int64\n",
      "7mer                    object\n",
      "PreTime                float64\n",
      "PreSD                  float64\n",
      "PreMean                float64\n",
      "InTime                 float64\n",
      "InSD                   float64\n",
      "InMean                 float64\n",
      "PostTime               float64\n",
      "PostSD                 float64\n",
      "PostMean               float64\n",
      "gene_id                 object\n",
      "label                    int64\n",
      "n_reads                  int64\n",
      "7mer_emb                object\n",
      "set_type                object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 11027106\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Data Types:\")\n",
    "print(reads_df.dtypes)\n",
    "\n",
    "# Display the number of rows\n",
    "print(\"\\nNumber of Rows:\", len(reads_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e7ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed dataset to Dataset/first_step_processed_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "reads_df.to_parquet(\"Dataset/first_step_processed_dataset.parquet\", index=False)\n",
    "print(\"Saved processed dataset to Dataset/first_step_processed_dataset.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcacdec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 4. Dataset class\n",
      "Ending: 4. Dataset class\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4. Dataset class\n",
    "# =========================\n",
    "print(\"Starting: 4. Dataset class\")\n",
    "class MILReadDataset(Dataset):\n",
    "    def __init__(self, reads_df, n_reads_per_site=None):\n",
    "        \"\"\"\n",
    "        reads_df: DataFrame of read-level features with columns like\n",
    "                  ['transcript_id', 'transcript_position', '7mer_emb', 'dwell_-1', ...]\n",
    "        n_reads_per_site: int or None\n",
    "            - int: maximum number of reads per site (randomly sampled)\n",
    "            - None: use all reads\n",
    "        \"\"\"\n",
    "        self.n_reads_per_site = n_reads_per_site\n",
    "        # group by site (transcript_id, transcript_position)\n",
    "        self.groups = reads_df.groupby(['transcript_id','transcript_position'])\n",
    "        self.bags = list(self.groups.groups.keys())\n",
    "        self.reads_df = reads_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bags)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tid, pos = self.bags[idx]\n",
    "        g = self.groups.get_group((tid,pos))\n",
    "\n",
    "        # numeric features\n",
    "        numeric_feats = g[['PreTime','PreSD','PreMean',\n",
    "                           'InTime','InSD','InMean',\n",
    "                           'PostTime','PostSD','PostMean']].values.astype(np.float32)\n",
    "\n",
    "        # k-mer embedding\n",
    "        kmer_emb = np.stack(g['7mer_emb'].values)\n",
    "        \n",
    "        # concatenate numeric + embedding\n",
    "        bag = np.concatenate([numeric_feats, kmer_emb], axis=1)\n",
    "\n",
    "        # ------------------------\n",
    "        # Handle n_reads_per_site\n",
    "        # ------------------------\n",
    "        if self.n_reads_per_site is not None and bag.shape[0] > self.n_reads_per_site:\n",
    "            # randomly sample n_reads_per_site reads\n",
    "            indices = np.random.choice(bag.shape[0], self.n_reads_per_site, replace=False)\n",
    "            bag = bag[indices]\n",
    "\n",
    "        # label for the bag\n",
    "        label = g['label'].iloc[0]\n",
    "        \n",
    "        return torch.tensor(bag), torch.tensor(label, dtype=torch.float32), tid, pos\n",
    "print(\"Ending: 4. Dataset class\")\n",
    "\n",
    "# =========================\n",
    "# 5. Imbalanced sampler\n",
    "# =========================\n",
    "class ImbalancedBagSampler(Sampler):\n",
    "    def __init__(self, dataset):\n",
    "        labels = np.array([dataset[i][1].item() for i in range(len(dataset))])\n",
    "        pos_idx = np.where(labels==1)[0]\n",
    "        neg_idx = np.where(labels==0)[0]\n",
    "        self.indices = np.concatenate([np.random.choice(pos_idx, size=len(neg_idx), replace=True), neg_idx])\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8738e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 6. Split Train/Val/Test\n",
      "Loading in first_step_processed_dataset.parquet\n",
      "Ending: 6. Split Train/Val/Test\n",
      "Starting: 7. Attention MIL Model\n",
      "Ending: 7. Attention MIL Model\n",
      "Starting: 8. Training Loop\n",
      "Epoch 1/100: TrainLoss=0.5031, ValROC-AUC=0.8686, ValPR-AUC=0.2742, Time=544.06s\n",
      "Saved best model (ValPR-AUC improved to 0.2742) at epoch 1\n",
      "Epoch 2/100: TrainLoss=0.4759, ValROC-AUC=0.8774, ValPR-AUC=0.2859, Time=452.43s\n",
      "Saved best model (ValPR-AUC improved to 0.2859) at epoch 2\n",
      "Epoch 3/100: TrainLoss=0.4630, ValROC-AUC=0.8790, ValPR-AUC=0.3119, Time=447.26s\n",
      "Saved best model (ValPR-AUC improved to 0.3119) at epoch 3\n",
      "Epoch 4/100: TrainLoss=0.4515, ValROC-AUC=0.8869, ValPR-AUC=0.3419, Time=453.23s\n",
      "Saved best model (ValPR-AUC improved to 0.3419) at epoch 4\n",
      "Epoch 5/100: TrainLoss=0.4459, ValROC-AUC=0.8941, ValPR-AUC=0.3710, Time=452.71s\n",
      "Saved best model (ValPR-AUC improved to 0.3710) at epoch 5\n",
      "Epoch 6/100: TrainLoss=0.4402, ValROC-AUC=0.8871, ValPR-AUC=0.3398, Time=448.96s\n",
      "No improvement in PR-AUC for 1 epoch(s).\n",
      "Epoch 7/100: TrainLoss=0.4351, ValROC-AUC=0.8918, ValPR-AUC=0.3708, Time=445.29s\n",
      "No improvement in PR-AUC for 2 epoch(s).\n",
      "Epoch 8/100: TrainLoss=0.4324, ValROC-AUC=0.8906, ValPR-AUC=0.3530, Time=451.58s\n",
      "No improvement in PR-AUC for 3 epoch(s).\n",
      "Epoch 9/100: TrainLoss=0.4312, ValROC-AUC=0.8967, ValPR-AUC=0.3700, Time=451.87s\n",
      "No improvement in PR-AUC for 4 epoch(s).\n",
      "Epoch 10/100: TrainLoss=0.4295, ValROC-AUC=0.8977, ValPR-AUC=0.3491, Time=448.81s\n",
      "No improvement in PR-AUC for 5 epoch(s).\n",
      "Epoch 11/100: TrainLoss=0.4286, ValROC-AUC=0.8955, ValPR-AUC=0.3685, Time=447.41s\n",
      "No improvement in PR-AUC for 6 epoch(s).\n",
      "Epoch 12/100: TrainLoss=0.4251, ValROC-AUC=0.8918, ValPR-AUC=0.3851, Time=454.18s\n",
      "Saved best model (ValPR-AUC improved to 0.3851) at epoch 12\n",
      "Epoch 13/100: TrainLoss=0.4258, ValROC-AUC=0.9010, ValPR-AUC=0.3758, Time=452.68s\n",
      "No improvement in PR-AUC for 1 epoch(s).\n",
      "Epoch 14/100: TrainLoss=0.4211, ValROC-AUC=0.8784, ValPR-AUC=0.3281, Time=450.70s\n",
      "No improvement in PR-AUC for 2 epoch(s).\n",
      "Epoch 15/100: TrainLoss=0.4204, ValROC-AUC=0.8926, ValPR-AUC=0.3516, Time=446.04s\n",
      "No improvement in PR-AUC for 3 epoch(s).\n",
      "Epoch 16/100: TrainLoss=0.4217, ValROC-AUC=0.8916, ValPR-AUC=0.3117, Time=452.49s\n",
      "No improvement in PR-AUC for 4 epoch(s).\n",
      "Epoch 17/100: TrainLoss=0.4185, ValROC-AUC=0.8941, ValPR-AUC=0.3575, Time=453.17s\n",
      "No improvement in PR-AUC for 5 epoch(s).\n",
      "Epoch 18/100: TrainLoss=0.4176, ValROC-AUC=0.9025, ValPR-AUC=0.3978, Time=450.47s\n",
      "Saved best model (ValPR-AUC improved to 0.3978) at epoch 18\n",
      "Epoch 19/100: TrainLoss=0.4145, ValROC-AUC=0.9028, ValPR-AUC=0.3906, Time=448.50s\n",
      "No improvement in PR-AUC for 1 epoch(s).\n",
      "Epoch 20/100: TrainLoss=0.4128, ValROC-AUC=0.9041, ValPR-AUC=0.3539, Time=452.79s\n",
      "No improvement in PR-AUC for 2 epoch(s).\n",
      "Epoch 21/100: TrainLoss=0.4123, ValROC-AUC=0.9020, ValPR-AUC=0.3913, Time=453.41s\n",
      "No improvement in PR-AUC for 3 epoch(s).\n",
      "Epoch 22/100: TrainLoss=0.4113, ValROC-AUC=0.8923, ValPR-AUC=0.2895, Time=451.54s\n",
      "No improvement in PR-AUC for 4 epoch(s).\n",
      "Epoch 23/100: TrainLoss=0.4130, ValROC-AUC=0.8890, ValPR-AUC=0.3687, Time=450.19s\n",
      "No improvement in PR-AUC for 5 epoch(s).\n",
      "Epoch 24/100: TrainLoss=0.4117, ValROC-AUC=0.9001, ValPR-AUC=0.3978, Time=451.95s\n",
      "No improvement in PR-AUC for 6 epoch(s).\n",
      "Epoch 25/100: TrainLoss=0.4122, ValROC-AUC=0.9041, ValPR-AUC=0.3699, Time=451.43s\n",
      "No improvement in PR-AUC for 7 epoch(s).\n",
      "Epoch 26/100: TrainLoss=0.4098, ValROC-AUC=0.9018, ValPR-AUC=0.3523, Time=452.29s\n",
      "No improvement in PR-AUC for 8 epoch(s).\n",
      "Epoch 27/100: TrainLoss=0.4098, ValROC-AUC=0.9053, ValPR-AUC=0.3986, Time=449.31s\n",
      "Saved best model (ValPR-AUC improved to 0.3986) at epoch 27\n",
      "Epoch 28/100: TrainLoss=0.4104, ValROC-AUC=0.9048, ValPR-AUC=0.3892, Time=462.32s\n",
      "No improvement in PR-AUC for 1 epoch(s).\n",
      "Epoch 29/100: TrainLoss=0.4108, ValROC-AUC=0.8887, ValPR-AUC=0.2597, Time=449.04s\n",
      "No improvement in PR-AUC for 2 epoch(s).\n",
      "Epoch 30/100: TrainLoss=0.4098, ValROC-AUC=0.9048, ValPR-AUC=0.3744, Time=453.98s\n",
      "No improvement in PR-AUC for 3 epoch(s).\n",
      "Epoch 31/100: TrainLoss=0.4096, ValROC-AUC=0.9051, ValPR-AUC=0.3977, Time=452.57s\n",
      "No improvement in PR-AUC for 4 epoch(s).\n",
      "Epoch 32/100: TrainLoss=0.4094, ValROC-AUC=0.9070, ValPR-AUC=0.3863, Time=452.29s\n",
      "No improvement in PR-AUC for 5 epoch(s).\n",
      "Epoch 33/100: TrainLoss=0.4063, ValROC-AUC=0.8936, ValPR-AUC=0.3203, Time=449.33s\n",
      "No improvement in PR-AUC for 6 epoch(s).\n",
      "Epoch 34/100: TrainLoss=0.4047, ValROC-AUC=0.8983, ValPR-AUC=0.3526, Time=453.65s\n",
      "No improvement in PR-AUC for 7 epoch(s).\n",
      "Epoch 35/100: TrainLoss=0.4072, ValROC-AUC=0.9041, ValPR-AUC=0.3632, Time=455.26s\n",
      "No improvement in PR-AUC for 8 epoch(s).\n",
      "Epoch 36/100: TrainLoss=0.4094, ValROC-AUC=0.8943, ValPR-AUC=0.3341, Time=453.41s\n",
      "No improvement in PR-AUC for 9 epoch(s).\n",
      "Epoch 37/100: TrainLoss=0.4062, ValROC-AUC=0.9040, ValPR-AUC=0.3991, Time=449.69s\n",
      "Saved best model (ValPR-AUC improved to 0.3991) at epoch 37\n",
      "Epoch 38/100: TrainLoss=0.4140, ValROC-AUC=0.9042, ValPR-AUC=0.4153, Time=451.72s\n",
      "Saved best model (ValPR-AUC improved to 0.4153) at epoch 38\n",
      "Epoch 39/100: TrainLoss=0.4347, ValROC-AUC=0.8911, ValPR-AUC=0.2458, Time=458.79s\n",
      "No improvement in PR-AUC for 1 epoch(s).\n",
      "Epoch 40/100: TrainLoss=0.4295, ValROC-AUC=0.7950, ValPR-AUC=0.1654, Time=451.74s\n",
      "No improvement in PR-AUC for 2 epoch(s).\n",
      "Epoch 41/100: TrainLoss=0.4224, ValROC-AUC=0.8966, ValPR-AUC=0.3455, Time=452.10s\n",
      "No improvement in PR-AUC for 3 epoch(s).\n",
      "Epoch 42/100: TrainLoss=0.4110, ValROC-AUC=0.9044, ValPR-AUC=0.3320, Time=449.47s\n",
      "No improvement in PR-AUC for 4 epoch(s).\n",
      "Epoch 43/100: TrainLoss=0.4124, ValROC-AUC=0.8442, ValPR-AUC=0.2283, Time=458.52s\n",
      "No improvement in PR-AUC for 5 epoch(s).\n",
      "Epoch 44/100: TrainLoss=0.4094, ValROC-AUC=0.8967, ValPR-AUC=0.2936, Time=453.31s\n",
      "No improvement in PR-AUC for 6 epoch(s).\n",
      "Epoch 45/100: TrainLoss=0.4096, ValROC-AUC=0.9064, ValPR-AUC=0.3688, Time=450.66s\n",
      "No improvement in PR-AUC for 7 epoch(s).\n",
      "Epoch 46/100: TrainLoss=0.4108, ValROC-AUC=0.9070, ValPR-AUC=0.3813, Time=449.73s\n",
      "No improvement in PR-AUC for 8 epoch(s).\n",
      "Epoch 47/100: TrainLoss=0.4019, ValROC-AUC=0.9109, ValPR-AUC=0.4060, Time=456.44s\n",
      "No improvement in PR-AUC for 9 epoch(s).\n",
      "Epoch 48/100: TrainLoss=0.4030, ValROC-AUC=0.9011, ValPR-AUC=0.3434, Time=455.22s\n",
      "No improvement in PR-AUC for 10 epoch(s).\n",
      "Early stopping at epoch 48 (no PR-AUC improvement for 10 epochs).\n",
      "Ending: 8. Training Loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 6. Split Train/Val/Test\n",
    "# =========================\n",
    "print(\"Starting: 6. Split Train/Val/Test\")\n",
    "\n",
    "print(\"Loading in first_step_processed_dataset.parquet\")\n",
    "reads_df = pd.read_parquet(\"Dataset/first_step_processed_dataset.parquet\")\n",
    "\n",
    "# Now split by set_type\n",
    "train_df = reads_df[reads_df['set_type'] == 'Train']\n",
    "val_df   = reads_df[reads_df['set_type'] == 'Val']\n",
    "test_df  = reads_df[reads_df['set_type'] == 'Test']\n",
    "\n",
    "train_ds = MILReadDataset(train_df, 20)\n",
    "val_ds   = MILReadDataset(val_df, 20)\n",
    "test_ds  = MILReadDataset(test_df, 20)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, sampler=ImbalancedBagSampler(train_ds))\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "print(\"Ending: 6. Split Train/Val/Test\")\n",
    "\n",
    "# =========================\n",
    "# 7. Attention MIL Model\n",
    "# =========================\n",
    "print(\"Starting: 7. Attention MIL Model\")\n",
    "class AttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim=25, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.instance_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, bag):\n",
    "        H = self.instance_encoder(bag)  \n",
    "        A = torch.softmax(self.attention(H), dim=0)  \n",
    "        M = torch.sum(A * H, dim=0)  \n",
    "        out = torch.sigmoid(self.classifier(M))\n",
    "        return out, A\n",
    "\n",
    "model = AttentionMIL(input_dim=25, hidden_dim=64).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(\"Ending: 7. Attention MIL Model\")\n",
    "\n",
    "# =========================\n",
    "# 8. Training Loop\n",
    "# =========================\n",
    "print(\"Starting: 8. Training Loop\")\n",
    "\n",
    "\n",
    "# ---- Hyperparameters ----\n",
    "n_epochs = 100\n",
    "early_stop_patience = 10  # stop if no PR-AUC improvement for 5 epochs\n",
    "best_val_pr = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "# ---- Tracking Metrics ----\n",
    "metrics_dict = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'val_roc_auc': [],\n",
    "    'val_pr_auc': [],\n",
    "    'epoch_time_sec': []\n",
    "}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for bag, label, _, _ in train_loader:\n",
    "        bag = bag[0].to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, attn = model(bag)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for bag, label, _, _ in val_loader:\n",
    "            bag = bag[0].to(device)\n",
    "            label = label.to(device)\n",
    "            out, attn = model(bag)\n",
    "\n",
    "            val_preds.append(out.item())\n",
    "            val_labels.append(label.item())\n",
    "\n",
    "    # ---- Compute Metrics ----        \n",
    "    val_auc = roc_auc_score(val_labels, val_preds)\n",
    "    val_pr  = average_precision_score(val_labels, val_preds)\n",
    "\n",
    "    # End timer\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # Log\n",
    "    metrics_dict['epoch'].append(epoch+1)\n",
    "    metrics_dict['train_loss'].append(total_loss)\n",
    "    metrics_dict['val_roc_auc'].append(val_auc)\n",
    "    metrics_dict['val_pr_auc'].append(val_pr)\n",
    "    metrics_dict['epoch_time_sec'].append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "          f\"TrainLoss={total_loss:.4f}, \"\n",
    "          f\"ValROC-AUC={val_auc:.4f}, \"\n",
    "          f\"ValPR-AUC={val_pr:.4f}, \"\n",
    "          f\"Time={epoch_time:.2f}s\")\n",
    "\n",
    "    # ---- Early Stopping & Model Saving ----\n",
    "    if val_pr > best_val_pr:\n",
    "        best_val_pr = val_pr\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f\"Models/epoch{epoch+1}_valpr{val_pr:.2f}.pth\")\n",
    "\n",
    "        print(f\"Saved best model (ValPR-AUC improved to {val_pr:.4f}) at epoch {epoch+1}\")\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in PR-AUC for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no PR-AUC improvement for {early_stop_patience} epochs).\")\n",
    "            break\n",
    "\n",
    "print(\"Ending: 8. Training Loop\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9864f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 9. Testing & Output\n",
      "Saved predictions to output.csv\n",
      "Ending: 9. Testing & Output\n",
      "Starting: 10. Evaluation Metrics\n",
      "Test ROC AUC: 0.8924, Test PR AUC: 0.4458\n",
      "Saved evaluation metrics to evaluation_metrics.csv\n",
      "Ending: 10. Evaluation Metrics\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Testing & Output\n",
    "# =========================\n",
    "print(\"Starting: 9. Testing & Output\")\n",
    "model.load_state_dict(torch.load(\"Models/epoch38_valpr0.42.pth\"))\n",
    "model.eval()\n",
    "output_rows = []\n",
    "y_test_true = []\n",
    "y_test_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bag, label, tid, pos in test_loader:\n",
    "        bag = bag[0].to(device)\n",
    "        label = label.to(device)\n",
    "        out, attn = model(bag)\n",
    "        output_rows.append({'transcript_id': tid[0], 'transcript_position': pos.item(), 'score': out.item()})\n",
    "        y_test_true.append(label.item())\n",
    "        y_test_pred.append(out.item())\n",
    "\n",
    "output_df = pd.DataFrame(output_rows)\n",
    "output_df.to_csv(\"output.csv\", index=False)\n",
    "print(\"Saved predictions to output.csv\")\n",
    "print(\"Ending: 9. Testing & Output\")\n",
    "\n",
    "# =========================\n",
    "# 10. Evaluation Metrics\n",
    "# =========================\n",
    "print(\"Starting: 10. Evaluation Metrics\")\n",
    "test_roc_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "test_pr_auc  = average_precision_score(y_test_true, y_test_pred)\n",
    "\n",
    "metrics_df = pd.DataFrame([{'test_roc_auc': test_roc_auc, 'test_pr_auc': test_pr_auc}])\n",
    "metrics_df.to_csv(\"evaluation_metrics.csv\", index=False)\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}, Test PR AUC: {test_pr_auc:.4f}\")\n",
    "print(\"Saved evaluation metrics to evaluation_metrics.csv\")\n",
    "print(\"Ending: 10. Evaluation Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab8744c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c59dfd7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34026269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to metrics.csv as CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming metrics_dict already exists\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "# Define the file path for CSV\n",
    "file_path = 'metrics.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Dictionary saved to {file_path} as CSV.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
