{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62c9109",
   "metadata": {},
   "source": [
    "# Multiple Instance Learning (MIL) for Read-Level RNA Site Classification\n",
    "*A PyTorch implementation with bag-level training, imbalanced sampling, and efficient batching.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6b03f",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we implement a **Multiple Instance Learning (MIL)** framework for read-level RNA site classification.\n",
    "\n",
    "- Each *bag* corresponds to a transcript site (defined by `transcript_id` and `transcript_position`).\n",
    "- Each *instance* corresponds to a single sequencing read belonging to that site.\n",
    "- Each bag has a **single binary label** (`0` or `1`), while individual reads are unlabeled.\n",
    "\n",
    "We train the model to predict the bag label by aggregating per-read predictions using a pooling mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Imports & Setup\n",
    "# =========================\n",
    "# === Standard Libraries ===\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# === Data Manipulation ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "# === Sklearn Metrics & Utilities ===\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Visualization ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "torch.manual_seed(RNG)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d64f95",
   "metadata": {},
   "source": [
    "### Configuration & Parameters\n",
    "\n",
    "This section defines all dataset paths, model hyperparameters, and training options.  \n",
    "Adjust these values as needed, then simply **Run All** to execute the full pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0. Configuration & Parameters\n",
    "# =========================\n",
    "\n",
    "# ---- General Settings ----\n",
    "RNG = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---- Data Paths ----\n",
    "DATA_DIR = \"../Dataset\"\n",
    "DATA_FILE = \"processed_dataset_lat2_432_epoch600_trained.parquet\"\n",
    "MODEL_DIR = \"Models\"\n",
    "RESULT_DIR = \"Results\"\n",
    "\n",
    "# ---- Data Sampling ----\n",
    "N_READS_PER_SITE = 20\n",
    "USE_DELTA = False\n",
    "NUMERIC_SCALER = None   # None uses StandardScalar(), else put in other scaler function\n",
    "KMER_SCALAR = None      # None uses StandardScalar(), else put in other scaler function\n",
    "POS_NEG_RATIO = 0.5     # oversample positives to 50% of negatives\n",
    "EFFECTIVE_RATIO = 1.0   # weighting ratio between pos:neg\n",
    "\n",
    "# ---- Model Parameters ----\n",
    "HIDDEN_DIM = 128\n",
    "DROPOUT = 0.2\n",
    "POOLING_METHOD = \"noisy-or\"  # [\"noisy-or\", \"mean\", \"max\"]\n",
    "\n",
    "# ---- Optimizer & Training ----\n",
    "LR = 1e-3\n",
    "N_EPOCHS = 300\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "BATCH_SIZE = 200\n",
    "BUCKET_SIZE = 1000\n",
    "\n",
    "# ---- Evaluation ----\n",
    "SAVE_PR_CURVE = True\n",
    "MODEL_SELECTION_METRIC = \"valscore\"  # options: valscore, valpr, rocauc\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e132c06",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "Each read is represented by numeric features (signal statistics) and sequence-based embeddings (5-mer context).\n",
    "\n",
    "**Columns:**\n",
    "- `transcript_id`, `transcript_position` → define bag identity\n",
    "- `label` → bag-level binary label\n",
    "- Numeric features: `PreTime`, `PreSD`, `PreMean`, ..., `PostMean`\n",
    "- K-mer embeddings: `Pre_5mer_*`, `In_5mer_*`, `Post_5mer_*`\n",
    "\n",
    "The processed dataset is stored in:\n",
    "DATA_DIR/DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_df = pd.read_parquet(f\"{DATA_DIR}/{DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95ecc4",
   "metadata": {},
   "source": [
    "## 3. Dataset Class: MILReadDataset\n",
    "\n",
    "This class organizes reads into bags and scales their features, with an **optional delta feature toggle**.\n",
    "\n",
    "**Key Features:**\n",
    "- Groups reads by `(transcript_id, transcript_position)` to form bags.\n",
    "- Applies `StandardScaler` normalization to numeric and k-mer features.\n",
    "- Supports **optional delta computation** between pre/in/post signal windows:\n",
    "  - Controlled by `use_delta` flag in the constructor.\n",
    "- Returns for each bag:\n",
    "  - `bag_read_level`: tensor of shape `(n_reads, feature_dim)`\n",
    "  - `label`: scalar bag label\n",
    "  - `tid`, `pos`: identifiers for tracking\n",
    "\n",
    "**Returned sample structure:**\n",
    "```python\n",
    "(\n",
    "  torch.tensor(bag_read_level, dtype=torch.float32),\n",
    "  torch.tensor(label, dtype=torch.float32),\n",
    "  tid, pos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacdec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Dataset class\n",
    "# =========================\n",
    "print(\"Starting: 3. Dataset class\")\n",
    "\n",
    "class MILReadDataset(Dataset):\n",
    "    def __init__(self, reads_df, n_reads_per_site=None, numeric_scaler=None, kmer_scaler=None, use_delta=False):\n",
    "        \"\"\"\n",
    "        reads_df: DataFrame with read-level rows:\n",
    "                  ['transcript_id','transcript_position','label',\n",
    "                   'PreTime','PreSD','PreMean','InTime','InSD','InMean',\n",
    "                   'PostTime','PostSD','PostMean','7mer_emb','7mer_emb_reduced']\n",
    "        n_reads_per_site: int or None\n",
    "            - int: max number of reads per bag\n",
    "        numeric_scaler: fitted StandardScaler for numeric features (if None, fit on full dataframe)\n",
    "        kmer_scaler: fitted StandardScaler for kmer embeddings (if None, fit on full dataframe)\n",
    "        use_delta: bool\n",
    "            - If True, adds delta (difference) features between Pre/In/Post regions\n",
    "        \"\"\"\n",
    "        self.n_reads_per_site = n_reads_per_site\n",
    "        self.use_delta = use_delta\n",
    "        self.groups = reads_df.groupby(['transcript_id', 'transcript_position'])\n",
    "        self.bags = list(self.groups.groups.keys())\n",
    "        self.reads_df = reads_df.copy()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Define numeric and kmer columns\n",
    "        # -----------------------------\n",
    "        self.numeric_cols = [\n",
    "            'PreTime','PreSD','PreMean','InTime','InSD','InMean','PostTime','PostSD','PostMean'\n",
    "        ]\n",
    "        self.kmer_cols = [col for col in reads_df.columns if col.startswith(('Pre_5mer_', 'In_5mer_', 'Post_5mer_'))]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Fit or use provided scalers\n",
    "        # -----------------------------\n",
    "        if numeric_scaler is None:\n",
    "            self.numeric_scaler = StandardScaler()\n",
    "            self.reads_df[self.numeric_cols] = self.numeric_scaler.fit_transform(self.reads_df[self.numeric_cols])\n",
    "        else:\n",
    "            self.numeric_scaler = numeric_scaler\n",
    "            self.reads_df[self.numeric_cols] = self.numeric_scaler.transform(self.reads_df[self.numeric_cols])\n",
    "\n",
    "        if kmer_scaler is None:\n",
    "            self.kmer_scaler = StandardScaler()\n",
    "            self.reads_df[self.kmer_cols] = self.kmer_scaler.fit_transform(self.reads_df[self.kmer_cols])\n",
    "        else:\n",
    "            self.kmer_scaler = kmer_scaler\n",
    "            self.reads_df[self.kmer_cols] = self.kmer_scaler.transform(self.reads_df[self.kmer_cols])\n",
    "\n",
    "        # -----------------------------\n",
    "        # Bag lengths and labels\n",
    "        # -----------------------------\n",
    "        self.bag_lengths = {k: len(v) for k, v in self.groups}\n",
    "        self.bag_labels = {}\n",
    "        for k in self.bags:\n",
    "            g = self.groups.get_group(k)\n",
    "            self.bag_labels[k] = int(g['label'].iloc[0])\n",
    "\n",
    "        # Infer read feature dimension\n",
    "        dummy_bag = self[0]\n",
    "        bag_feats, _, _, _ = dummy_bag\n",
    "        self.read_dim = bag_feats.shape[1]\n",
    "        print(f\"✅ MILReadDataset initialized: read_dim={self.read_dim}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bags)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tid, pos = self.bags[idx]\n",
    "        g = self.groups.get_group((tid, pos))\n",
    "\n",
    "        read_parts = []\n",
    "\n",
    "        # Numeric features (already scaled)\n",
    "        numeric_feats = g[self.numeric_cols].values.astype(np.float32)\n",
    "        read_parts.append(numeric_feats)\n",
    "\n",
    "        # Optional delta features\n",
    "        if self.use_delta:\n",
    "            deltas = []\n",
    "            deltas.append(numeric_feats[:, 3] - numeric_feats[:, 0])  # InTime - PreTime\n",
    "            deltas.append(numeric_feats[:, 6] - numeric_feats[:, 3])  # PostTime - InTime\n",
    "            deltas.append(numeric_feats[:, 4] - numeric_feats[:, 1])  # InSD - PreSD\n",
    "            deltas.append(numeric_feats[:, 7] - numeric_feats[:, 4])  # PostSD - InSD\n",
    "            deltas.append(numeric_feats[:, 5] - numeric_feats[:, 2])  # InMean - PreMean\n",
    "            deltas.append(numeric_feats[:, 8] - numeric_feats[:, 5])  # PostMean - InMean\n",
    "            delta_feats = np.stack(deltas, axis=1).astype(np.float32)\n",
    "            read_parts.append(delta_feats)\n",
    "\n",
    "        # Embedded 5-mer features\n",
    "        kmer_feats = g[self.kmer_cols].values.astype(np.float32)\n",
    "        read_parts.append(kmer_feats)\n",
    "\n",
    "        # Combine all read-level features\n",
    "        bag_read_level = np.concatenate(read_parts, axis=1)\n",
    "\n",
    "        # Random subsampling\n",
    "        if self.n_reads_per_site is not None and bag_read_level.shape[0] > self.n_reads_per_site:\n",
    "            idxs = np.random.choice(bag_read_level.shape[0], self.n_reads_per_site, replace=False)\n",
    "            bag_read_level = bag_read_level[idxs]\n",
    "\n",
    "        label = int(g['label'].iloc[0])\n",
    "\n",
    "        return (torch.tensor(bag_read_level, dtype=torch.float32),\n",
    "                torch.tensor(label, dtype=torch.float32),\n",
    "                tid, pos)\n",
    "\n",
    "print(\"Ending: 3. Dataset class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441ef1a",
   "metadata": {},
   "source": [
    "## 4. Bucketed Batch Sampling and Collation\n",
    "\n",
    "Bags vary in the number of reads. To improve computational efficiency:\n",
    "\n",
    "- **BucketBatchSampler** sorts bags by length and groups them into \"buckets\" of similar length.\n",
    "- **Within each bucket:** random shuffling ensures stochasticity.\n",
    "- **Collate function:** dynamically pads all bags in a batch to the maximum bag length, minimizing wasted computation.\n",
    "\n",
    "This approach mimics variable-length batching in NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Imbalanced sampler\n",
    "# =========================\n",
    "print(\"Starting: 4. Samplers and Collate\")\n",
    "class ImbalancedBagSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Oversamples positive bags to balance classes, with rebalancing every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, pos_neg_ratio=1.0):\n",
    "        \"\"\"\n",
    "        dataset: your MILReadDataset\n",
    "        balance_ratio: ratio of positive to negative samples after oversampling.\n",
    "                       - 1.0 = fully balanced (default)\n",
    "                       - 0.5 = positives are half as many as negatives\n",
    "                       - >1.0 = more positives than negatives\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.pos_neg_ratio = pos_neg_ratio\n",
    "        # Pre-cache labels to avoid repeated dataset access\n",
    "        self.bags = self.dataset.bags\n",
    "        self.labels = np.array([self.dataset.bag_labels[k] for k in self.bags], dtype=np.int64)\n",
    "        self.pos_idx = np.where(self.labels == 1)[0]\n",
    "        self.neg_idx = np.where(self.labels == 0)[0]\n",
    "\n",
    "        if len(self.pos_idx) == 0:\n",
    "            raise ValueError(\"No positive bags found in dataset; pos_neg_ratio sampling not possible.\")\n",
    "        \n",
    "    def sample_indices(self):\n",
    "        n_pos_target = int(len(self.neg_idx) * self.pos_neg_ratio)\n",
    "        sampled_pos = np.random.choice(self.pos_idx, size=n_pos_target, replace=True)\n",
    "        combined = np.concatenate([self.neg_idx, sampled_pos])\n",
    "        np.random.shuffle(combined)\n",
    "        return combined\n",
    "    def __iter__(self):\n",
    "        return iter(self.sample_indices())\n",
    "\n",
    "    def __len__(self):\n",
    "        # length in terms of number of sampled bag indices\n",
    "        return len(self.neg_idx) + int(len(self.neg_idx) * self.pos_neg_ratio)\n",
    "    \n",
    "class BucketBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Groups bags of similar lengths (based on n_reads) into buckets,\n",
    "    shuffles bucket order each epoch, and yields random batches.\n",
    "    Please at the least set bucket_size value to be multiple of batch_size\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, base_sampler, batch_size=50, bucket_size=200):\n",
    "        self.dataset = dataset\n",
    "        self.base_sampler = base_sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Get sampled indices from ImbalancedBagSampler\n",
    "        indices = self.base_sampler.sample_indices()\n",
    "        lengths = np.array([self.dataset.bag_lengths[self.dataset.bags[i]] for i in indices])\n",
    "        sorted_idx = indices[np.argsort(lengths)]\n",
    "\n",
    "        # Split into buckets of similar lengths\n",
    "        buckets = [sorted_idx[i:i+self.bucket_size] for i in range(0, len(sorted_idx), self.bucket_size)]\n",
    "        np.random.shuffle(buckets)\n",
    "\n",
    "        lst_of_batch = []\n",
    "        for bucket in buckets:\n",
    "            np.random.shuffle(bucket)\n",
    "            for i in range(0, len(bucket), self.batch_size):\n",
    "                batch = bucket[i:i+self.batch_size]\n",
    "                lst_of_batch.append(batch)\n",
    "        np.random.shuffle(lst_of_batch)\n",
    "\n",
    "        for batch in lst_of_batch:\n",
    "            yield batch.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        total = len(self.base_sampler.neg_idx) + int(len(self.base_sampler.neg_idx) * self.base_sampler.pos_neg_ratio)\n",
    "        return max(1, total // self.batch_size)\n",
    "\n",
    "# Collate function for minimal padding\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (bag_read_level, label, tid, pos)\n",
    "    Returns:\n",
    "      padded_feats: (B, max_len, feat_dim)\n",
    "      labels: (B,)\n",
    "      tids, positions\n",
    "    \"\"\"\n",
    "    bag_read_level_list, labels, tids, positions = zip(*batch)\n",
    "\n",
    "    max_len = max(x.shape[0] for x in bag_read_level_list)\n",
    "    feat_dim = bag_read_level_list[0].shape[1]\n",
    "    padded_feats = torch.zeros(len(bag_read_level_list), max_len, feat_dim, dtype=torch.float32)\n",
    "    for i, nf in enumerate(bag_read_level_list):\n",
    "        padded_feats[i, :nf.shape[0], :] = nf\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return padded_feats, labels, tids, positions\n",
    "\n",
    "print(\"Ending: 4. Samplers and Collate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364490c",
   "metadata": {},
   "source": [
    "## 5. Model Architecture: MILReadOnly\n",
    "\n",
    "This model applies a *read-level encoder* followed by a *bag-level pooling mechanism*.\n",
    "\n",
    "### **Components**\n",
    "- **ReadEncoder:** 3-layer residual MLP with LayerNorm and GELU activations.\n",
    "- **Bag Predictor:** Linear layer projecting read embeddings → per-read logits.\n",
    "\n",
    "### **Pooling Options**\n",
    "1. **Noisy-OR** – probabilistic aggregation:\n",
    "   \\[\n",
    "   P(\\text{bag positive}) = 1 - \\prod_{i=1}^N (1 - P_i)\n",
    "   \\]\n",
    "2. **Mean pooling** – average of read probabilities  \n",
    "3. **Max pooling** – selects the most confident read probability\n",
    "\n",
    "### **Output Flow**\n",
    "| Step | Shape | Description |\n",
    "|------|--------|-------------|\n",
    "| Reads | (B, N, read_dim) | Input reads per bag |\n",
    "| Encoded reads | (B, N, hidden_dim) | Instance representations |\n",
    "| Read logits | (B, N) | Read-level scores |\n",
    "| Bag probability | (B,) | Aggregated bag prediction |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c730a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Attention MIL Model\n",
    "# =========================\n",
    "print(\"Starting: 5. Attention MIL Model\")\n",
    "class ReadEncoder(nn.Module):\n",
    "    def __init__(self, read_dim, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(read_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h) + h\n",
    "        h = self.block3(h) + h\n",
    "        return h\n",
    "class MILReadOnly(nn.Module):\n",
    "    def __init__(self, read_dim, hidden_dim=128, dropout=0.2, pooling=\"noisy-or\"):\n",
    "        \"\"\"\n",
    "        read_dim: dimension of per-read feature vector\n",
    "        hidden_dim: hidden dim for read encoder\n",
    "        dropout: dropout probability\n",
    "        pooling: str, one of [\"noisy-or\", \"mean\", \"max\"]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert pooling in [\"noisy-or\", \"mean\", \"max\", \"learnable\"], \\\n",
    "            \"pooling must be one of 'noisy-or', 'mean', 'max'\"\n",
    "        self.pooling_type = pooling\n",
    "\n",
    "        # Per-read encoder\n",
    "        self.read_encoder = ReadEncoder(read_dim, hidden_dim, dropout)\n",
    "\n",
    "\n",
    "        # Bag-level predictor\n",
    "        self.bag_predictor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, bag_read_level):\n",
    "        \"\"\"\n",
    "        bag_read_level: (B, N, read_dim)\n",
    "        Returns:\n",
    "            bag_logits: (B,)\n",
    "            read_probs or attention weights: (B, N)\n",
    "        \"\"\"\n",
    "        B, N, _ = bag_read_level.shape\n",
    "\n",
    "        # Encode reads\n",
    "        read_feats = self.read_encoder(bag_read_level)  # (B, N, hidden_dim)\n",
    "\n",
    "        # Per-read logits for pooling methods\n",
    "        read_logits = self.bag_predictor(read_feats).squeeze(-1)  # (B, N)\n",
    "        read_probs = torch.sigmoid(read_logits)\n",
    "\n",
    "        if self.pooling_type == \"noisy-or\":\n",
    "            bag_probs = 1 - torch.prod(1 - read_probs, dim=1)\n",
    "        elif self.pooling_type == \"mean\":\n",
    "            bag_probs = read_probs.mean(dim=1)\n",
    "        elif self.pooling_type == \"max\":\n",
    "            bag_probs, _ = read_probs.max(dim=1)\n",
    "\n",
    "        attn_weights = read_probs  # just to return something consistent\n",
    "        bag_logits = torch.log(bag_probs / (1 - bag_probs + 1e-7) + 1e-7)\n",
    "\n",
    "        return bag_logits, attn_weights\n",
    "\n",
    "# ---------------------------\n",
    "# Focal Loss (unchanged)\n",
    "# ---------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction=\"none\")\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_factor = (1 - pt) ** self.gamma\n",
    "        loss = self.alpha * focal_factor * bce_loss\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "def predict_probs(model, bag_read_level):\n",
    "    logits, read_probs = model(bag_read_level)\n",
    "    return torch.sigmoid(logits), read_probs\n",
    "\n",
    "print(\"Ending: 5. Attention MIL Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1849f72",
   "metadata": {},
   "source": [
    "## 6a. Split Train / Val / Test\n",
    "\n",
    "This step loads the dataset from disk and splits it into Train, Validation, and Test subsets using the set_type column.\n",
    "\n",
    "The splits are wrapped in MILReadDataset objects.\n",
    "\n",
    "train_loader is generated fresh each epoch (inside the training loop) to support dynamic oversampling of positive samples.\n",
    "\n",
    "val_loader and test_loader are initialized once with batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6a. Split Train/Val/Test \n",
    "# =========================\n",
    "print(\"Starting: 6a. Split Train/Val/Test\")\n",
    "\n",
    "reads_df = pd.read_parquet(f\"../Dataset/{DATA_FILE}\")\n",
    "print(f\"Loading in {DATA_FILE}\")\n",
    "\n",
    "# Now split by set_type\n",
    "train_df = reads_df[reads_df['set_type'] == 'Train']\n",
    "val_df   = reads_df[reads_df['set_type'] == 'Val']\n",
    "test_df  = reads_df[reads_df['set_type'] == 'Test']\n",
    "\n",
    "train_ds = MILReadDataset(train_df, n_reads_per_site= N_READS_PER_SITE, numeric_scaler=NUMERIC_SCALER, kmer_scaler=KMER_SCALAR, use_delta=USE_DELTA) \n",
    "val_ds = MILReadDataset(val_df, n_reads_per_site = N_READS_PER_SITE, numeric_scaler=NUMERIC_SCALER, kmer_scaler=KMER_SCALAR, use_delta=USE_DELTA) \n",
    "test_ds = MILReadDataset(test_df, n_reads_per_site = N_READS_PER_SITE, numeric_scaler=NUMERIC_SCALER, kmer_scaler=KMER_SCALAR, use_delta=USE_DELTA)\n",
    "\n",
    "#train_loader is inside Epoch Loop so as to randomise the positve bags that are oversampled\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "print(\"Ending: 6a. Split Train/Val/Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958a4c4",
   "metadata": {},
   "source": [
    "## 6b. Define Model and Optimizer\n",
    "\n",
    "In this step:\n",
    "\n",
    "The read_dim (number of input features per read) is automatically inferred from the training dataset.\n",
    "\n",
    "A MILReadOnly model is initialized using this read_dim, a hidden_dim of 128, and dropout of 0.2.\n",
    "\n",
    "An Adam optimizer is used with a slightly higher learning rate (1e-3) for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6b. Define model + optimizer\n",
    "# =========================\n",
    "# Get read and site dimensions from dataset\n",
    "print(\"Starting: 6b. Define model + optimizer\")\n",
    "\n",
    "read_dim = train_ds.read_dim\n",
    "\n",
    "print(f\"Automatically detected read_dim={read_dim}\")\n",
    "\n",
    "# Initialize model using the inferred dimensions\n",
    "model = MILReadOnly(read_dim=read_dim, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, pooling=POOLING_METHOD).to(device)\n",
    "\n",
    "# change the lr to have higher penalisation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Ending: 6b. Define model + optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9ae9d",
   "metadata": {},
   "source": [
    "## 7. Training Pipeline\n",
    "\n",
    "Each epoch:\n",
    "1. Rebuilds a new **oversampled training loader** using fresh positive sampling.\n",
    "2. Trains over all bags using MIL loss.\n",
    "3. Evaluates validation AUC and PR-AUC.\n",
    "4. Implements **early stopping** based on sum of ROC-AUC and PR-AUC improvement.\n",
    "5. Saves the model checkpoint each epoch.\n",
    "\n",
    "**Key metrics:**\n",
    "- Training loss\n",
    "- Validation ROC-AUC\n",
    "- Validation PR-AUC\n",
    "- Epoch time and average time per bag\n",
    "\n",
    "\n",
    "### **Handling Class Imbalance**\n",
    "\n",
    "RNA modification sites are typically imbalanced — far more negatives than positives.\n",
    "\n",
    "We use **two strategies** to handle imbalance:\n",
    "\n",
    "1. **Oversampling** (data-level correction):  \n",
    "   Implemented via `ImbalancedBagSampler`, which resamples positive bags with replacement\n",
    "   according to a configurable positive-to-negative ratio (`pos_neg_ratio`).\n",
    "\n",
    "2. **Loss weighting** (loss-level correction):  \n",
    "   The `BCEWithLogitsLoss` includes a `pos_weight` parameter to penalize misclassified positives more strongly.\n",
    "\n",
    "Together, this ensures balanced gradients and effective learning even under strong label imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8738e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. Training Loop\n",
    "# =========================\n",
    "print(\"Starting: 7. Training Loop\")\n",
    "\n",
    "# ---- Hyperparameters ----\n",
    "best_val_score = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "# Count raw labels\n",
    "pos_count = sum(label.item() for _, label, _, _ in train_ds)\n",
    "neg_count = len(train_ds) - pos_count\n",
    "\n",
    "# Effective counts\n",
    "effective_pos_count = int(neg_count * POS_NEG_RATIO)\n",
    "effective_neg_count = neg_count\n",
    "\n",
    "print(f\"Original positives: {pos_count}, negatives: {neg_count}\")\n",
    "print(f\"Effective positives (oversampling {POS_NEG_RATIO*100:.0f}%): {effective_pos_count}\")\n",
    "\n",
    "# Loss weighting\n",
    "pos_weight_value = (effective_neg_count / max(effective_pos_count, 1)) * EFFECTIVE_RATIO\n",
    "pos_weight = torch.tensor(pos_weight_value).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "# criterion = FocalLoss(alpha=pos_weight_value, gamma=2.0)\n",
    "\n",
    "# Sampler\n",
    "sampler = ImbalancedBagSampler(train_ds, pos_neg_ratio=POS_NEG_RATIO)\n",
    "\n",
    "# ---- Tracking Metrics ----\n",
    "metrics_dict = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'val_roc_auc': [],\n",
    "    'val_pr_auc': [],\n",
    "    'epoch_time_sec': [],\n",
    "    'avg_time_per_bag': []\n",
    "}\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Rebuild dataloader and batch order each epoch with a fresh sampler\n",
    "    # Please reduce just use bucket_size that is a multiple of the batch_size\n",
    "    batch_sampler = BucketBatchSampler(train_ds, sampler, batch_size=BATCH_SIZE, bucket_size=BUCKET_SIZE)\n",
    "    train_loader = DataLoader(train_ds, batch_sampler=batch_sampler, collate_fn=collate_fn)\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_bags = 0\n",
    "    for bag_read_level, label, _, _ in train_loader:\n",
    "        n_bags += 1\n",
    "        bag_read_level = bag_read_level.to(device)\n",
    "        label = label.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outs, attns = model(bag_read_level)\n",
    "        \n",
    "        loss = criterion(outs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # if n_bags % 50000 == 0:\n",
    "        #     elapsed = time.time() - epoch_start\n",
    "        #     print(f\"  Processed {n_bags} bags, avg time per bag: {elapsed / n_bags:.4f}s\")\n",
    "\n",
    "    total_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bag_read_level, label, _, _ in val_loader:\n",
    "            bag_read_level = bag_read_level.to(device)\n",
    "            label = label.to(device).view(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            out, _ = predict_probs(model, bag_read_level)\n",
    "\n",
    "            # Convert predictions and labels to list for metrics calculation\n",
    "            val_preds.extend(out.detach().cpu().numpy())\n",
    "            val_labels.extend(label.detach().cpu().numpy())\n",
    "            \n",
    "    # ---- Compute Metrics ----        \n",
    "    val_auc = roc_auc_score(val_labels, val_preds)\n",
    "    val_pr  = average_precision_score(val_labels, val_preds)\n",
    "\n",
    "    # End timer\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_time_per_bag = epoch_time / n_bags\n",
    "\n",
    "    # Log\n",
    "    metrics_dict['epoch'].append(epoch+1)\n",
    "    metrics_dict['train_loss'].append(total_loss)\n",
    "    metrics_dict['val_roc_auc'].append(val_auc)\n",
    "    metrics_dict['val_pr_auc'].append(val_pr)\n",
    "    metrics_dict['epoch_time_sec'].append(epoch_time)\n",
    "    metrics_dict['avg_time_per_bag'].append(avg_time_per_bag)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}: \"\n",
    "          f\"TrainLoss={total_loss:.4f}, \"\n",
    "          f\"ValROC-AUC={val_auc:.4f}, \"\n",
    "          f\"ValPR-AUC={val_pr:.4f}, \"\n",
    "          f\"EpochTime={epoch_time:.2f}s,\"\n",
    "          f\"AvgTimePerBag={avg_time_per_bag:.4f}s\")\n",
    "\n",
    "    # ---- Early Stopping & Model Saving ----\n",
    "    val_score = val_auc + val_pr  # combined metric\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{MODEL_DIR}/epoch{epoch+1}_rocauc{val_auc:.3f}_valpr{val_pr:.3f}_valscore{val_score:.3f}.pth\")\n",
    "    print(f\"Saved model (ValROC-AUC={val_auc:.4f}, ValPR-AUC={val_pr:.4f}, Score={val_score:.4f}) at epoch {epoch+1}\")\n",
    "\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in combined score for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement for {EARLY_STOP_PATIENCE} epochs).\")\n",
    "            break\n",
    "\n",
    "print(\"Ending: 7. Training Loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e122c",
   "metadata": {},
   "source": [
    "Saving training set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f78361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming metrics_dict already exists\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(metrics_dict)\n",
    "\n",
    "# Define the file path for CSV\n",
    "file_path = 'metrics.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Dictionary saved to {file_path} as CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ba39d",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "\n",
    "After training, the model is evaluated using:\n",
    "\n",
    "- **ROC-AUC (Receiver Operating Characteristic Area Under Curve):**  \n",
    "  Measures discrimination between positive and negative bags.\n",
    "  \n",
    "- **PR-AUC (Precision-Recall Area Under Curve):**  \n",
    "  More sensitive to class imbalance, especially when positives are rare.\n",
    "\n",
    "Both are computed at the **bag level** since labels are per bag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_eval_model(\n",
    "    model_path,\n",
    "    device=\"cpu\",\n",
    "    test_loader=None,\n",
    "    manual_model_file=None,\n",
    "    is_full_test=False,\n",
    "    save_pr_curve=False,\n",
    "    selection_metric=\"valscore\"  # NEW\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and evaluate a model. Auto-selects the best model based on specified metric.\n",
    "    \"\"\"\n",
    "\n",
    "    valid_metrics = {\"valscore\", \"valpr\", \"rocauc\"}\n",
    "    if selection_metric not in valid_metrics:\n",
    "        raise ValueError(f\"Invalid selection_metric. Choose from {valid_metrics}\")\n",
    "\n",
    "    # --- Get model file ---\n",
    "    if manual_model_file:\n",
    "        model_file = manual_model_file\n",
    "        print(f\"Using manually specified model: {model_file}\")\n",
    "    else:\n",
    "        pattern = r\"epoch(\\d+)_rocauc([\\d.]+)_valpr([\\d.]+)_valscore([\\d.]+)\\.pth\"\n",
    "        models = []\n",
    "\n",
    "        for f in os.listdir(model_path):\n",
    "            match = re.match(pattern, f)\n",
    "            if match:\n",
    "                epoch = int(match.group(1))\n",
    "                rocauc = float(match.group(2))\n",
    "                valpr = float(match.group(3))               \n",
    "                valscore = float(match.group(4))\n",
    "\n",
    "                metric_value = {\n",
    "                    \"valscore\": valscore,\n",
    "                    \"valpr\": valpr,\n",
    "                    \"rocauc\": rocauc\n",
    "                }[selection_metric]\n",
    "\n",
    "                models.append((f, metric_value, epoch))\n",
    "\n",
    "        if not models:\n",
    "            raise ValueError(\"No valid model files found in directory.\")\n",
    "\n",
    "        # Sort by selected metric descending, then epoch descending\n",
    "        models.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "        model_file = models[0][0]\n",
    "\n",
    "        print(f\"Auto-selected best model by '{selection_metric}': {model_file} \"\n",
    "              f\"({selection_metric} = {models[0][1]:.4f}, epoch = {models[0][2]})\")\n",
    "\n",
    "    # --- Load the model ---\n",
    "    model = MILReadOnly(\n",
    "        read_dim=test_loader.dataset.read_dim,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        dropout=DROPOUT,\n",
    "        pooling=POOLING_METHOD\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path, model_file), map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    output_rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bag_read_level, label, tid, pos in test_loader:\n",
    "            bag_read_level = bag_read_level.to(device)\n",
    "            label = label.to(device).view(-1)\n",
    "\n",
    "            out, _ = model(bag_read_level)\n",
    "            prob = torch.sigmoid(out).item()\n",
    "\n",
    "            y_true.append(label.item())\n",
    "            y_pred.append(prob)\n",
    "\n",
    "            output_rows.append({\n",
    "                \"transcript_id\": tid[0],\n",
    "                \"transcript_position\": pos.item(),\n",
    "                \"score\": prob\n",
    "            })\n",
    "\n",
    "    # --- Metrics ---\n",
    "    roc = roc_auc_score(y_true, y_pred)\n",
    "    pr  = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"[Evaluation] ROC-AUC={roc:.4f}, PR-AUC={pr:.4f}\")\n",
    "\n",
    "    # --- Save predictions ---\n",
    "    model_name = os.path.splitext(model_file)[0]\n",
    "    output_file = f\"{RESULT_DIR}/{'full_test' if is_full_test else 'test'}_data_output_{model_name}.csv\"\n",
    "    pd.DataFrame(output_rows).to_csv(output_file, index=False)\n",
    "    print(f\"Saved predictions to: {output_file}\")\n",
    "\n",
    "    # --- Save PR Curve ---\n",
    "    if save_pr_curve:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, label=f'PR-AUC = {pr:.4f}', color='blue')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        pr_curve_file = f\"{RESULT_DIR}/{'full_test' if is_full_test else 'test'}_pr_curve_{model_name}.png\"\n",
    "        plt.savefig(pr_curve_file)\n",
    "        plt.close()\n",
    "        print(f\"Saved Precision-Recall curve to: {pr_curve_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7060d54",
   "metadata": {},
   "source": [
    "## 9. Saving and Inference\n",
    "\n",
    "Each model is saved under the `MODEL_DIR` directory with the naming pattern: {MODEL_DIR}/epoch{epoch_number}_rocauc{val_auc:.3f}_valpr{val_pr:.3f}_valscore{val_score:.3f}.pth\n",
    "\n",
    "For inference, use:\n",
    "```python\n",
    "model.load_state_dict(torch.load(f\"{MODEL_DIR}/best_model.pth\"))\n",
    "probs, read_weights = predict_probs(model, bag_tensor)\n",
    "```\n",
    "`probs` = bag-level probability\n",
    "\n",
    "`read_weights` = per-read importance estimates (e.g., for interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_eval_model(\n",
    "    MODEL_DIR,\n",
    "    device=device,\n",
    "    test_loader=test_loader,\n",
    "    save_pr_curve=SAVE_PR_CURVE,\n",
    "    selection_metric=MODEL_SELECTION_METRIC\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306dbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_and_eval_model(\n",
    "#     model_path=MODEL_DIR,\n",
    "#     device=\"cuda\",\n",
    "#     test_loader=test_loader,\n",
    "#     manual_model_file=\"epoch53_valpr0.42.pth\",\n",
    "#     save_pr_curve= SAVE_PR_CURVE\n",
    "#     selection_metric=\"valscore\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading in {DATA_FILE} for testing on full data\")\n",
    "reads_df = pd.read_parquet(f\"../Dataset/{DATA_FILE}\")\n",
    "\n",
    "test_full_ds = MILReadDataset(reads_df, n_reads_per_site = N_READS_PER_SITE, numeric_scaler=NUMERIC_SCALER, kmer_scaler=KMER_SCALAR, use_delta=USE_DELTA)\n",
    "\n",
    "test_full_loader  = DataLoader(test_full_ds, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9464a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_eval_model(\n",
    "    model_path=MODEL_DIR,\n",
    "    device=device,\n",
    "    test_loader=test_full_loader,\n",
    "    is_full_test= True,\n",
    "    save_pr_curve=SAVE_PR_CURVE,\n",
    "    selection_metric=MODEL_SELECTION_METRIC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02709c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 10. Summary\n",
    "\n",
    "✅ **This pipeline implements true Multiple Instance Learning:**\n",
    "- Each bag (site) consists of multiple unlabeled reads (instances).\n",
    "- The model outputs read-level probabilities, then pools them into one bag probability.\n",
    "- The bag prediction is compared against the bag-level label during training.\n",
    "\n",
    "✅ **Core Features**\n",
    "| Feature | Description |\n",
    "|----------|--------------|\n",
    "| MIL formulation | Proper bag-instance hierarchy |\n",
    "| Pooling | Noisy-OR / mean / max |\n",
    "| Imbalance handling | Oversampling + weighted loss |\n",
    "| Batching | Bucketed variable-length batching |\n",
    "| Architecture | Read encoder + bag predictor |\n",
    "\n",
    "This notebook demonstrates a full end-to-end MIL setup for biological read-level data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
