{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2748a75d",
   "metadata": {},
   "source": [
    "## 0. Imports & Setup\n",
    "\n",
    "This section sets up the environment:\n",
    "\n",
    "- Imports all necessary packages (standard libraries, pandas, NumPy, etc.).\n",
    "- Ensures reproducibility by seeding `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056e5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0a. Imports & Setup\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility\n",
    "RNG = 42\n",
    "np.random.seed(RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a201414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ðŸ”§ 0b. Configuration\n",
    "# =========================\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"Raw File/dataset0.csv\"\n",
    "os.makedirs(\"Dataset\", exist_ok= True)\n",
    "output_parquet_path = \"Dataset/before_embedding.parquet\"\n",
    "\n",
    "# Column names from raw file\n",
    "rename_columns = {\n",
    "    'ID': 'transcript_id',\n",
    "    'POS': 'transcript_position',\n",
    "    'SEQ': '7mer'\n",
    "}\n",
    "\n",
    "split_ratios = {\n",
    "    'Train': 0.8,\n",
    "    'Val': 0.1,\n",
    "    'Test': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310750f",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Dataset\n",
    "\n",
    "- Loads the raw dataset from a CSV file.\n",
    "- Drops unnecessary or redundant columns.\n",
    "- Renames specific columns for clarity and consistency:\n",
    "  - `'ID'` â†’ `'transcript_id'`\n",
    "  - `'POS'` â†’ `'transcript_position'`\n",
    "  - `'SEQ'` â†’ `'7mer'`\n",
    "- Computes the number of reads per `(transcript_id, transcript_position)` pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5a13cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Load and Prepare Dataset\n",
      "âœ… Dataset loaded and prepared.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Load and Preparing datasets\n",
    "# =========================\n",
    "print(\"Starting: Load and Prepare Dataset\")\n",
    "\n",
    "# Load\n",
    "reads_df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Clean up\n",
    "columns_to_drop = ['Unnamed: 0']\n",
    "reads_df = reads_df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "reads_df = reads_df.rename(columns=rename_columns)\n",
    "\n",
    "# Add read count per transcript-position\n",
    "reads_df['n_reads'] = reads_df.groupby(['transcript_id', 'transcript_position'])['7mer'].transform('size')\n",
    "\n",
    "print(\"âœ… Dataset loaded and prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedc40b",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Extract 5-mers from 7-mer\n",
    "\n",
    "This step processes each 7-mer sequence into three overlapping 5-mers:\n",
    "\n",
    "- **Pre-5mer**: bases 0â€“4  \n",
    "- **In-5mer**: bases 1â€“5  \n",
    "- **Post-5mer**: bases 2â€“6\n",
    "\n",
    "These 5-mers will be used later for feature embedding or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5b1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 5-mer extraction\n",
      "âœ… 5-mer columns added.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 2. Preprocessing: 7-mer embedding\n",
    "# =========================\n",
    "print(\"Starting: 5-mer extraction\")\n",
    "\n",
    "def extract_5mers(seq):\n",
    "    pre_5mer = seq[0:5]   # bases 0-4\n",
    "    in_5mer = seq[1:6]    # bases 1-5\n",
    "    post_5mer = seq[2:7]  # bases 2-6\n",
    "    return pre_5mer, in_5mer, post_5mer\n",
    "\n",
    "# Apply extraction\n",
    "reads_df[['Pre_5mer', 'In_5mer', 'Post_5mer']] = reads_df['7mer'].apply(\n",
    "    lambda x: pd.Series(extract_5mers(x))\n",
    ")\n",
    "\n",
    "print(\"âœ… 5-mer columns added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bf8a2",
   "metadata": {},
   "source": [
    "## 3. Assign Train / Val / Test Splits (By Gene)\n",
    "\n",
    "- Splits the dataset into **Train**, **Validation**, and **Test** sets **by gene**, not randomly by rows.\n",
    "- Ensures all rows from the same gene appear in only one of the three sets.\n",
    "- Maintains label balance across the splits using a greedy bin-filling strategy.\n",
    "- Target split ratios:\n",
    "  - Train: 80%\n",
    "  - Validation: 10%\n",
    "  - Test: 10%\n",
    "- Prints the number of rows per set and label distribution (% of label 0 and 1).\n",
    "- Saves the result to a `.parquet` file for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "870725c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 3. Assign split bins\n",
      "ðŸ“Š Number of rows in each set:\n",
      "  - Train: 8820055 rows\n",
      "  - Val: 1105069 rows\n",
      "  - Test: 1101982 rows\n",
      "\n",
      "ðŸ“ˆ Label distribution:\n",
      "  - Test: 0 = 94.08%, 1 = 5.92%\n",
      "  - Train: 0 = 95.59%, 1 = 4.41%\n",
      "  - Val: 0 = 95.90%, 1 = 4.10%\n",
      "Ending: 3. Assign split bins\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3. Assign split bins\n",
    "# =========================\n",
    "print(\"Starting: 3. Assign split bins\")\n",
    "\n",
    "def assign_set_type_by_gene(reads_df, split_ratios={'Train': 0.8, 'Val': 0.1, 'Test': 0.1}, random_state=42):\n",
    "    \"\"\"\n",
    "    Assigns each row in reads_df a 'set_type' of Train, Val, or Test,\n",
    "    ensuring all rows with the same gene_id are in the same set,\n",
    "    and total number of rows (not just genes) in each set matches desired ratios.\n",
    "    Label distribution is approximately balanced using a greedy strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Get stats per gene\n",
    "    gene_stats = (\n",
    "        reads_df\n",
    "        .groupby('gene_id')['label']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .rename(columns={0: 'label_0', 1: 'label_1'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    gene_stats['total'] = gene_stats['label_0'] + gene_stats['label_1']\n",
    "\n",
    "    # Shuffle genes for randomness\n",
    "    gene_stats = gene_stats.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Overall label distribution and target row counts\n",
    "    total_rows = gene_stats['total'].sum()\n",
    "    total_label_1 = gene_stats['label_1'].sum()\n",
    "    overall_pos_rate = total_label_1 / total_rows\n",
    "\n",
    "    target_rows = {k: total_rows * split_ratios[k] for k in split_ratios}\n",
    "\n",
    "    # Step 3: Initialize bins\n",
    "    bins = {\n",
    "        'Train': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "        'Val': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "        'Test': {'genes': [], 'label_0': 0, 'label_1': 0, 'total': 0},\n",
    "    }\n",
    "\n",
    "    def pick_bin():\n",
    "        # Find the bin with the biggest gap between current and target row count\n",
    "        diffs = {k: target_rows[k] - bins[k]['total'] for k in bins}\n",
    "        # Choose the bin that needs rows the most\n",
    "        return max(diffs, key=diffs.get)\n",
    "\n",
    "    # Step 4: Assign genes to bins to match row targets and label balance\n",
    "    for _, row in gene_stats.iterrows():\n",
    "        chosen_bin = pick_bin()\n",
    "        bins[chosen_bin]['genes'].append(row['gene_id'])\n",
    "        bins[chosen_bin]['label_0'] += row['label_0']\n",
    "        bins[chosen_bin]['label_1'] += row['label_1']\n",
    "        bins[chosen_bin]['total'] += row['total']\n",
    "\n",
    "    # Step 5: Map gene_id â†’ set_type\n",
    "    gene_to_set = {}\n",
    "    for set_name, bin_data in bins.items():\n",
    "        for gene_id in bin_data['genes']:\n",
    "            gene_to_set[gene_id] = set_name\n",
    "\n",
    "    reads_df['set_type'] = reads_df['gene_id'].map(gene_to_set)\n",
    "\n",
    "    return reads_df\n",
    "\n",
    "\n",
    "reads_df = assign_set_type_by_gene(reads_df, split_ratios = split_ratios, random_state=RNG)\n",
    "\n",
    "set_counts = reads_df['set_type'].value_counts()\n",
    "print(\"ðŸ“Š Number of rows in each set:\")\n",
    "for set_name, count in set_counts.items():\n",
    "    print(f\"  - {set_name}: {count} rows\")\n",
    "\n",
    "# Print label distribution per set (normalized)\n",
    "\n",
    "label_dist = reads_df.groupby('set_type')['label'].value_counts(normalize=True).unstack()\n",
    "print(\"\\nðŸ“ˆ Label distribution:\")\n",
    "for set_name in label_dist.index:\n",
    "    print(f\"  - {set_name}: 0 = {label_dist.loc[set_name].get(0, 0)*100:.2f}%, 1 = {label_dist.loc[set_name].get(1, 0)*100:.2f}%\")\n",
    "\n",
    "print(\"Ending: 3. Assign split bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d472d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved processed data to: Dataset/before_embedding.parquet\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4. Save Preprocessed Dataset\n",
    "# =========================\n",
    "\n",
    "reads_df.to_parquet(output_parquet_path, index=False)\n",
    "print(f\"âœ… Saved processed data to: {output_parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
